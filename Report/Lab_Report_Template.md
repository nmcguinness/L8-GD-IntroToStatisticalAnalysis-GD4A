# User Testing Lab Report
## 3D Game Development - Group Continuous Assessment

**Team Name:**  
**Game Title:**  
**Student Name(s):**  
**Date:**  

---

> **Report Length:** Maximum 2000-3000 words (excludes tables, figures, references, and appendices)

---

## Executive Summary

> **Guidance:** Write this section last. It should be a standalone summary (150-200 words) that captures your entire study. A busy stakeholder should be able to read only this and understand what you did, what you found, and what you recommend.

**Purpose of Testing:**  
**Key Findings (Summary):**  
**Recommendations (Summary):**

> **Guidance:** Provide a high-level overview (150-200 words) written last. Summarize your purpose, methods, key findings, and most important recommendations. This should be concise but comprehensive enough that someone could understand your main points without reading the full report.

---

## 1. Introduction

> **Guidance:** This section provides context for your study. Summarize the game's genre, platform, and development stage. State what specifically is being tested across both sessions (e.g., tutorial in Week 7, revised tutorial in Week 11). Outline specific questions or goals and state your null and alternative hypotheses in plain English.

**Game Context & Background:**  

> Describe your game briefly: genre, platform, target audience, and current development stage (alpha, beta, etc.).

**Scope of Testing:**  

> State what aspects of the game were tested in each session. Be specific about which levels, mechanics, or features were the focus.

**Testing Objectives:**  

> List 2-4 specific questions you wanted to answer through testing. Link each to a game design decision or concern.

**Hypotheses (H₀/H₁):**

> State your null and alternative hypotheses in plain English, not statistical jargon.
> 
> *Example hypothesis:*  
> - **H₀:** There is no difference in tutorial completion time between Build A and Build B.  
> - **H₁:** Build B reduces tutorial completion time compared to Build A.

---

## 2. Methodology

> **Guidance:** This section explains exactly how you conducted your user testing. Be specific enough that another team could replicate your study. Include details about both testing sessions (Week 7 and Week 11).

### 2.1 Test Design & Approach

> Describe the overall testing approach (usability testing, playtesting, think-aloud protocol). Explain whether sessions were remote or in-person, and note any differences between Week 7 and Week 11 sessions.

**Testing Sessions:**  
- Week 7: [describe first testing session]  
- Week 11: [describe second testing session]

**Test Scenarios / Tasks:**  

> List the specific tasks participants were asked to complete. Be precise (e.g., "Complete tutorial level from start to boss encounter" rather than "play the game").

**Metrics Collected:**

> List all quantitative and qualitative data collected (e.g., completion time, deaths, survey responses, verbal feedback).

### 2.2 Participants

> Explain who participated in your testing and how they were selected. Include demographic information if relevant to your research questions.

**Recruitment & Criteria:**  

> Describe how you found participants and any inclusion/exclusion criteria (e.g., "no prior experience with our game," "age 18+").

**Number & Demographics:**  
- Week 7: N = [X]  
- Week 11: N = [X]  

> Include relevant demographics: gaming experience, age range, platform familiarity, etc.

**Consent & Ethical Considerations:**

> State that participants gave informed consent and note any steps taken to protect privacy/confidentiality.

### 2.3 Materials & Tools

> Detail the technical setup and tools used for testing and data collection.

**Software / Hardware Specifications:**  

> List game build version, platform (PC/console), OS, recording software, etc.

**Testing Environment:**  

> Describe where testing occurred (lab, participants' homes, online via screen share, etc.).

**Data Capture Tools:**

> List tools used to collect data (Google Forms, Excel spreadsheets, OBS recording, etc.).

### 2.4 Procedure

> Provide a step-by-step walkthrough of what happened during each testing session.

**Test Session Flow:**

1. Welcome & consent forms
2. Brief explanation of game controls
3. Play session with observation/think-aloud
4. Post-test questionnaire
5. Brief interview

**Session Duration:**  

> State typical session length (e.g., "45-60 minutes total").

**Facilitator & Observer Roles:**

> Explain who ran the sessions and what each person did (e.g., "One team member facilitated while two observed and took notes").

---

## 3. Data Analysis

> **Guidance:** This section explains how you prepared and analyzed your data. Be transparent about any data cleaning decisions and clearly state which statistical methods you used.

### 3.1 Dataset Handling

> State any exclusions, data cleaning steps, or handling of missing values. Justify each decision.

**Exclusions & Data Cleaning:**

> **Required:** State what you excluded (e.g., incomplete sessions for time metrics) and why.
>
> *Example:* "We excluded 3 participants from Week 7 who did not complete Task 1, as time metrics cannot be calculated for incomplete sessions. Final sample: Week 7 N=27, Week 11 N=32."

### 3.2 Metrics Chosen

> Identify 2-4 primary metrics that directly address your testing objectives. Explain why each metric matters.

**Primary Metrics (2-4):**

1. [Metric 1 + units + link to testing objective]
2. [Metric 2 + units + link to testing objective]
3. [Metric 3 + units + link to testing objective]
4. [Metric 4 + units + link to testing objective]

> *Example:* "Task 1 completion time (seconds) - measures tutorial effectiveness"

### 3.3 Quantitative Methods Used

> List the statistical analyses you performed. At minimum, include descriptive statistics, one confidence interval, and a correlation matrix.

> **Required minimum components:**
> - Descriptive statistics (mean, median, SD, n)
> - At least one 95% confidence interval
> - Correlation matrix for key numeric metrics

**Statistical Analyses Performed:**

- Descriptive statistics for [list metrics]
- 95% confidence interval for [specify comparison]
- Correlation matrix for [list 3-5 key numeric variables]

### 3.4 Qualitative Analysis (if applicable)

> Explain how you analyzed open-ended feedback, if you collected any.

**Approach to Open-Ended Responses:**

> If you collected qualitative data, explain how you grouped comments (themes/categories) and what you counted or prioritized.

---

## 4. Results

> **Guidance:** This section presents your findings objectively. Report numbers, tables, and figures without extensive interpretation (save that for Discussion). Be precise and reference specific tables/figures in your text.

### 4.1 Summary Statistics

> Present your key metrics in table format. This is typically "Table 1" in academic reports.

**Table 1: Summary of Key Metrics by Build/Group**

| Metric | Build A | Build B | Difference |
|:-------|:--------|:--------|:-----------|
| n | [X] | [X] | - |
| Mean Task 1 Time (sec) | [X.X] | [X.X] | [X.X] |
| SD Task 1 Time (sec) | [X.X] | [X.X] | - |
| Median Task 1 Time (sec) | [X.X] | [X.X] | [X.X] |
| Task 1 Completion Rate | [X.X%] | [X.X%] | [X.X%] |
| Mean Satisfaction Score (1-7) | [X.X] | [X.X] | [X.X] |

> **Required:** Table 1 must include n, mean/median, and spread (SD or range) for each group/build.

### 4.2 Confidence Interval

> Report your 95% CI in a complete sentence that relates to gameplay, not just statistics.

**95% Confidence Interval for [Primary Comparison]:**

> **Required:** Report at least one 95% CI in gameplay terms.
>
> *Example:* "Build B reduced Task 1 completion time by **12.3 seconds** on average (95% CI [8.1, 16.5]). This suggests players navigated the revised tutorial significantly faster, likely due to the improved waypoint system. The confidence interval indicates this reduction is consistent and meaningful for gameplay experience."

### 4.3 Key Findings by Category

> Organize findings by the main categories you investigated (e.g., tutorial completion, UI usability, combat difficulty). For each category, report the metric, the finding, and a brief interpretation.

#### Category 1: [e.g., Tutorial Completion]

**Metric:** [e.g., Time to complete tutorial]

**Finding:** [Report numerical result with reference to Table 1 or specific analysis]

**Interpretation:** [2-4 sentences in gameplay terms explaining what this means for design and player experience]

#### Category 2: [e.g., User Interface]

**Metric:** [e.g., UI clarity rating]

**Finding:**

**Interpretation:**

#### Category 3: [Additional category if relevant]

**Metric:**

**Finding:**

**Interpretation:**

### 4.4 Visualizations

> Present 2-4 plots that tell the story of your data. Each plot should answer a specific question from your objectives.

> **Required:** 2-4 plots that support your story:
> - At least one distribution plot
> - At least one group comparison plot  
> - At least one plot encoding an extra dimension (color/fill/facets)

#### Figure 1: [Title - e.g., "Task 1 Completion Time by Build"]

> Show a comparison between groups/builds using boxplots, bar charts, or similar.

[INSERT PLOT: Group comparison boxplot or similar showing Build A vs Build B]

**Interpretation:** [2-4 sentences explaining what variables are shown, the pattern/trend, what it means for game design, and any recommendations]

> *Example interpretation:* "Figure 1 shows the distribution of Task 1 completion times for Build A (original tutorial) and Build B (revised tutorial). The median time decreased from 87 seconds to 68 seconds, with Build B showing less variability (tighter distribution). This confirms that the revised tutorial waypoint system improved both speed and consistency. We recommend keeping these waypoint improvements for the final build."

#### Figure 2: [Title - e.g., "Distribution of Tutorial Completion Times"]

> Show the shape of your data distribution using histograms or density plots.

[INSERT PLOT: Histogram or density plot showing distribution shape]

**Interpretation:** [2-4 sentences discussing shape of distribution (normal/skewed/bimodal), where most responses fall, outliers, and design implications]

#### Figure 3: [Title - e.g., "Task Time by Experience Level and Build"]

> Show an additional dimension (e.g., experience level, platform, session number) using color, facets, or grouped elements.

[INSERT PLOT: Faceted or grouped plot showing an extra dimension]

**Interpretation:** [2-4 sentences comparing patterns across different groups/categories and discussing design implications]

#### Figure 4: [Title - optional additional plot]

> Include a fourth plot if needed to support your conclusions.

[INSERT PLOT if needed]

**Interpretation:**

### 4.5 Comparison Analysis (if comparing builds/groups)

> If you compared two builds or groups, follow this 4-sentence structure from Notes 02.

> **If applicable:** Use the 4-sentence comparison template from Notes 02.

**Build/Group Comparison:**

1. **What we compared:** [State the comparison clearly]
   - *Example:* "We compared Task 1 completion time between Build A (Week 7) and Build B (Week 11)."

2. **What we found:** [Report difference + 95% CI]
   - *Example:* "Build B showed a mean reduction of 19.2 seconds (95% CI [14.3, 24.1])."

3. **Statistical significance:** [If tested, report p-value and interpretation]
   - *Example:* "The confidence interval excludes zero, indicating this difference is unlikely due to chance alone (p < 0.01)."

4. **Practical significance:** [What this means for gameplay/design]
   - *Example:* "This 22% reduction in tutorial time suggests the revised waypoint system successfully addressed player confusion identified in Week 7 testing."

### 4.6 Correlation Analysis

> Present a correlation matrix for your key numeric metrics and interpret the strongest relationships.

> **Required:** Include a correlation matrix showing relationships between 3-5 key numeric metrics.

**Table 2: Correlation Matrix for Key Metrics**

|  | Task Time | Deaths | Score | Satisfaction |
|:--|:---------|:-------|:------|:-------------|
| **Task Time** | 1.00 | [X.XX] | [X.XX] | [X.XX] |
| **Deaths** | [X.XX] | 1.00 | [X.XX] | [X.XX] |
| **Score** | [X.XX] | [X.XX] | 1.00 | [X.XX] |
| **Satisfaction** | [X.XX] | [X.XX] | [X.XX] | 1.00 |

**Top 3 Correlations:**

> Identify and interpret the three strongest correlations (highest absolute r values). Explain what each relationship means in gameplay terms.

1. [Variable pair + r value + gameplay interpretation]
   - *Example:* "Task time correlated strongly with deaths (r = 0.68), consistent with players repeating the same encounter after failure."

2. [Variable pair + r value + gameplay interpretation]

3. [Variable pair + r value + gameplay interpretation]

---

## 5. Discussion

> **Guidance:** This section interprets your findings and connects them back to your objectives. Discuss what the results mean for your game design, acknowledge limitations, and compare to expectations or previous testing.

**Interpretation of Findings:**

> Relate findings back to your initial objectives. Discuss whether goals were met and any unexpected results. Connect the numbers to design decisions.

**Key Insights:**

> Synthesize 3-5 major takeaways that emerged from your analysis. For each, explain why it matters for development.

1. [Major insight 1 connecting data to design decisions]
2. [Major insight 2 addressing research questions/hypotheses]
3. [Major insight 3 highlighting unexpected patterns]

**Limitations of the Study:**

> Honestly assess weaknesses in your study. Explain how these limitations might affect reliability or validity of findings.

- [Limitation 1 + how it affects reliability/validity]
  - *Example:* "Small sample size (N=27 in Week 7) limits generalizability. A larger sample would provide more precise confidence intervals."
  
- [Limitation 2 + how it affects interpretation]
  - *Example:* "All participants were recruited from the same class, creating potential selection bias toward experienced players."
  
- [How you might address these limitations in future research]
  - *Example:* "Future testing should include a more diverse player base with varied experience levels."

**Comparison to Previous Testing:**

> If Week 11 was a retest after implementing changes from Week 7, compare the results and discuss whether improvements were successful. If this is your first round of testing, omit this subsection.

---

## 6. Recommendations

> **Guidance:** Based on your data analysis, provide specific, actionable recommendations. Each should be supported by evidence and prioritized by importance and feasibility.

> **Required:** At least one concrete design/next test action based on evidence.

**Prioritized Improvements:**

> List 3-5 recommendations in priority order. For each, clearly link to supporting data and explain expected impact.

### 1. [High Priority]: [Specific Recommendation]

> This is your most important recommendation - the change most strongly supported by data that will have the biggest impact.

- **Evidence:** [Which data/findings support this - reference specific tables/figures]
- **Expected Impact:** [How this will improve player experience]
- **Implementation:** [Brief note on feasibility/challenges]

> *Example:*  
> **High Priority: Implement revised waypoint system in all tutorial levels**
> - **Evidence:** Build B reduced completion time by 22% (Table 1, Figure 1) and increased satisfaction scores from 4.2 to 5.8 (Table 1).
> - **Expected Impact:** Faster onboarding with less frustration for new players.
> - **Implementation:** Waypoint assets already created; requires level design updates to Levels 2-3.

### 2. [Medium Priority]: [Specific Recommendation]

> Important improvements that should be made but are less urgent than high-priority items.

- **Evidence:**
- **Expected Impact:**
- **Implementation:**

### 3. [Low Priority]: [Specific Recommendation]

> Nice-to-have improvements that could enhance experience but aren't critical.

- **Evidence:**
- **Expected Impact:**
- **Implementation:**

**Action Plan & Next Steps:**

> Provide a concrete roadmap for the development team. Include timeline if possible and any plans for follow-up testing.

> Provide a direct roadmap for the development team. Include plans for follow-up testing if relevant.

---

## 7. Conclusion

> **Guidance:** Provide a concise wrap-up of your entire study. Restate your most important findings and their implications without introducing new information.

**Summary of Findings:**

> Recap the overall impact of the user-testing results in 2-3 sentences. Focus on the most important takeaway.

> Recap the overall impact of the user-testing results. Restate the most important findings in 2-3 sentences.

**Value of Findings:**

> Explain how these findings will improve your game or inform development decisions.

> Reinforce the benefit of these findings for final product quality.

**Future Considerations:**

> Mention any next steps, upcoming features, or areas that might need additional testing.

> Mention any upcoming milestones, planned features, or areas that might need additional testing.

---

## 8. References

> **Guidance:** List any sources you cited in your report. Use a consistent citation format (APA recommended). Include methodologies, academic papers, or tools referenced in your methodology or discussion.

> List any methodologies, research papers, or tools you cited. Use consistent citation format (APA, IEEE, etc.).

- Nielsen, J. (1994). *Usability inspection methods.* John Wiley & Sons.
- [Additional references as needed]

---

## Appendices

> **Guidance:** Include supplementary materials that provide context but aren't essential to understanding your main findings. Appendices don't count toward the 1600-word limit.

### Appendix A: Raw Data & Analysis Files

> Provide links to your data files, analysis scripts, or statistical software outputs. If files are large, provide links to cloud storage.

> Include links to or descriptions of your data files, analysis scripts/spreadsheets, or statistical software outputs.

### Appendix B: Survey Instrument

> Include the complete text of your survey questions or playtest protocol so others could replicate your study.

> Include a copy of your complete survey questions or playtest protocol.

**Example Survey Questions:**

1. [Question 1]
2. [Question 2]
3. [etc.]

### Appendix C: Additional Visualizations

> Include any supplementary plots that support your findings but didn't fit in the main Results section.

> Include any supplementary plots or analyses that didn't fit in the main Results section.

### Appendix D: Detailed Statistical Output

> Include full output from your statistical analyses (e.g., complete regression tables, detailed correlation matrices).

> Include full statistical output for regression models, correlation matrices, or other analyses where you only reported key findings in Results.

---

## Minimum Requirements Checklist

Before submission, verify you have included:

- [ ] **Hypothesis** (H₀/H₁) in plain English
- [ ] **Summary table** with n, mean/median, spread for each group
- [ ] **At least one comparison** (mean/median difference or rate difference)
- [ ] **At least one 95% confidence interval** reported in gameplay terms
- [ ] **At least one group comparison plot** (boxplot or similar)
- [ ] **2-4 total plots** supporting your story
- [ ] **Interpretation** (2-4 sentences) for each major finding
- [ ] **At least one concrete recommendation** with supporting evidence
- [ ] **Statement of exclusions** (what data was excluded and why)
- [ ] **Correlation matrix** for key numeric metrics (mandatory)
- [ ] **Report length** ≤ 1600 words (excluding tables, figures, references, appendices)
